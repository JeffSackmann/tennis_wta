\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}

\usepackage{appendix}
\usepackage{apacite}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\numberwithin{equation}{section}

\author{}
\title{Tennis Project}
\date{}
\begin{document}
\maketitle

\section{The Bradley-Terry Model}

We denote $D$ to be the data of pairwise comparisons (matches) among $K$ individuals. To compare two elements $i$, $j$, the Bradley-Terry model suggests, 
\begin{align} \label{bradleyterry}
\mathbb{P}(i >j) = \frac{\lambda_i}{\lambda_i + \lambda_j},
\end{align} 
where $i>j$ means that element $i$ beats element $j$ and $\lambda = \{\lambda_i\}_{i=1}^K$ is parameters with $\lambda_i >0$ that represents the skill of $i$. \\

Let $w_{ij}$ denote the number of matches where $i$ beats $j$, then the total number of matches element $i$ wins is, $w_i = \sum_{j=1, j \neq i}^K w_{ij}$ and $n_{ij} = w_{ij}+w_{ji}$ is the total number of matches between individuals $i$ and $j$. Using equation \ref{bradleyterry}, we have log-likelihood, 
\begin{align}
l_D(\lambda) &= \sum _{1 \leq i\neq j \leq K} [w_{ij}\log\lambda_i - w_{ij}\log(\lambda_i+\lambda_j)] \\
&= \sum _{1 \leq i\neq j \leq K} w_{ij}\log\lambda_i - \sum _{1 \leq i< j \leq K} n_{ij}\log(\lambda_i+\lambda_j),
\end{align}
with notation $1 \leq i\neq j \leq K$ denotes the set $\{(i,j) \in \{1,\cdots, K\}^2 \ \mathrm{such \ that} \ i\neq j \}$ and $1 \leq i<j \leq K$ denotes the set $\{(i,j) \in \{1,\cdots, K\}^2 \ \mathrm{such \ that} \ i<j \}$. Trying to maximise this for $\lambda$  takes a long time to converge so we want to introduce a latent variable, say $z$, so the complete log-likelihood will have a simpler form. This is possible as, $p(z,D|\lambda) = p(z|D,\lambda)p(D|\lambda)$ meaning we can sum the log-likehoods,2 $l_{z,D}(\lambda) = l_z(\lambda) + l_D(\lambda)$.\\

For each $1 \leq i<j \leq K$ and for each match $k=1, \cdots n_{ij}$, let $Y_{ki} \sim \mathrm{Exp} (\lambda_i)$ and $Y_{kj} \sim \mathrm{Exp}(\lambda_j)$ where $\mathrm{Exp}(\lambda)$ is the exponential distribution with parameter $\lambda$ Therefore, 
\begin{align}
\mathbb{P}(Y_{ki} < Y_{kj}) = \frac{\lambda_i}{\lambda_i + \lambda_j},
\end{align}
we can think of $Y_{ki}$ and $Y_{kj}$ as 'clocks' where the individual whose clock goes off first as the winner. The more skilled a player is the more likely their clock will go off first. Now, for each $i,j$ pair we introduce latent random variable, 
\begin{align} \label{latentrv}
Z_{ij} &= \sum _{k=1}^{n_{ij}} \min (Y_{kj},Y_{ki}) \sim \mathrm{Gamma} (n_{ij}, \lambda _i + \lambda _j),\\
p(z|D,\lambda) &= \prod _{1 \leq i<j \leq K | n_{ij} >0} \mathrm{Gamma} (z_{ij} ; n_{ij}, \lambda_i + \lambda_j), \\
l_z(\lambda) &= \sum _{1 \leq i<j \leq K}[n_{ij}\log (\lambda_i + \lambda_j) - (\lambda_i + \lambda_j)z_{ij} + (n_ij -1)z_{ij} - \log \Gamma (n_{ij})]
\end{align}
$Z_{ij}$ has a gamma distribution because the minimum of two exponential  is exponentially distribution, $\min (Y_{kj},Y_{ki}) \sim \mathrm{Exp}(\lambda _i + \lambda _j)$ and the sum of identically distributed exponential random variables has a gamma distribution with the shape parameter being the number of random variables in the sum and the rate parameter is the parameter of the exponential distribution. \\ 

Therefore, we have complete log-likelihood, 
\begin{align}
l_c(\lambda) &= l_D(\lambda) + l_z(\lambda) \\
l_c(\lambda) &= \sum _{i=1}^K w_i\log \lambda_i - \sum _{1 \leq i<j \leq K | n_{ij} >0} [(\lambda_i + \lambda_j)z_{ij} - (n_ij -1)z_{ij} + \log \Gamma (n_{ij})].
\end{align}

If we also use a a prior, 
\begin{align}
p(\lambda) = \prod _{i=1}^K \mathrm{Gamma}(\lambda_i ; a,b),
\end{align}
with shape parameter $a>0$ and rate parameter $b>0$. We can then maximise using the EM algorithm, where for iteration, $t$,
\begin{align}
\lambda^{(t)} = \argmax _{\lambda} Q(\lambda,\lambda^{(t-1)})
\end{align}
where 
\begin{align}
Q(\lambda,\lambda^{(t-1)}) &= \mathbb{E}_{Z|D, \lambda^{(t-1)}} (l_c(\lambda)) + \log p(\lambda) \\
&= \sum _{i=1}^K[(a-1+w_i)\log \lambda_i -b\lambda_i] - \sum _{1 \leq i<j \leq K} \frac{n_{ij}(\lambda_i + \lambda_j)}{(\lambda_i^{(t-1)} + \lambda_j^{(t-1)})}
\end{align}
Using above, 
\begin{align}
\lambda_i ^{(t)} = \frac{a-1+w_i}{b + \sum_{j \neq i}\frac{n_{ij}}{\lambda_i^{(t-1)}+\lambda_j^{(t-1)}}}.
\end{align}

We can also sample from the posterior distribution $p(\lambda, z|D)$ using a data augmentation algorithm. We can update $Z|\lambda$ using equation \ref{latentrv} and $\lambda|Z$  using $p(\lambda|z,D) \propto p(\lambda)\exp (l_c(\lambda))$ , where $p(\lambda)$ is the conjugate prior to, 
\begin{align}
\exp (l_c(\lambda)) \propto \prod _{i=1}^K \left( \lambda_i^{w_i} \exp \left[ -\left(\sum_{i<j|n_{ij} >0} z_{ij} + \sum_{i>j|n_{ij} >0}z_{ji}\right)\lambda_i \right] \right).
\end{align}

The sampler at iteration $t$ is, 
\begin{itemize}
\item For $1 \leq i<j \leq K$ such that $n_{ij} >0$, 
\begin{align}
Z_{ij}^{(t)} |D,\lambda^{(t-1)} \sim \mathrm{Gamma} (n_{ij}, \lambda_i^{(t-1)} + \lambda_{j}^{(t-1)}).
\end{align}
\item For $i = 1, \cdots, K$, 
\begin{align}
\lambda_i^{(t)} | D,Z^{(t)} \sim \mathrm{Gamma} \left( a + w_i, b + \sum_{i<j|n_{ij} >0} Z_{ij} + \sum_{i>j|n_{ij} >0}Z_{ji} \right).
\end{align}
\end{itemize}
\end{document}